{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data source\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data to local machine\n",
    "import wget\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('data/car_data'):\n",
    "  if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "  wget.download(url=url, out='data/car_data', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin', 'Car']\n",
    "\n",
    "car= pd.read_table(url, header=None, sep='\\s+', names=cols, na_values='?')\n",
    "\n",
    "car.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataset\n",
    "\n",
    "data = car.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem statement:**    \n",
    "Our aim here is to predict the MPG value for a vehicle, given that we have other attributes of that vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Horsepower.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the horsepower\n",
    "\n",
    "sns.boxplot(x=data.Horsepower);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imputing the values with median\n",
    "median = data['Horsepower'].median()\n",
    "data['Horsepower'] = data['Horsepower'].fillna(median)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##category distribution\n",
    "\n",
    "data[\"Cylinders\"].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Origin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pairplots to get an intuition of potential correlations\n",
    "\n",
    "sns.pairplot(data[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\", \"Horsepower\"]], diag_kind=\"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop car type from the dataset\n",
    "car_type = data['Car']\n",
    "data = data.drop('Car', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying srarified splitting \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"Cylinders\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distribution \n",
    "print('training_dataset')\n",
    "print(strat_train_set['Cylinders'].value_counts() / len(strat_train_set))\n",
    "\n",
    "print()\n",
    "print('test_dataset')\n",
    "print(strat_test_set[\"Cylinders\"].value_counts() / len(strat_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strat_test_set.to_csv('ml_in_powerbi/data/test_data.csv', index=False)\n",
    "# strat_train_set.to_csv('ml_in_powerbi/data/training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### converting the Origin column to strings\n",
    "##converting integer classes to countries in Origin \n",
    "sample_data = strat_train_set.sample(10)\n",
    "sample_data['Origin'] = sample_data['Origin'].map({1: 'India', 2: 'USA', 3 : 'Germany'})\n",
    "sample_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##one hot encoding\n",
    "x = pd.get_dummies(strat_train_set, prefix='', prefix_sep='')\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing new variables by checking their correlation w.r.t. MPG\n",
    "x['displacement_on_power'] = x['Displacement'] / x['Horsepower']\n",
    "x['weight_on_cylinder'] = x['Weight'] / x['Cylinders']\n",
    "x['acceleration_on_power'] = x['Acceleration'] / x['Horsepower']\n",
    "x['acceleration_on_cyl'] = x['Acceleration'] / x['Cylinders']\n",
    "\n",
    "corr_matrix = x.corr()\n",
    "corr_matrix['MPG'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " numerics = ['float64', 'int64']\n",
    "x.select_dtypes(include=numerics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "acc_ix, hpower_ix, cyl_ix = 4, 2, 0\n",
    "\n",
    "##custom class inheriting the BaseEstimator and TransformerMixin\n",
    "class CustomAttrAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, acc_on_power=True):\n",
    "        self.acc_on_power = acc_on_power  # new optional variable\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        acc_on_cyl = X[:, acc_ix] / X[:, cyl_ix] # required new variable\n",
    "        if self.acc_on_power:\n",
    "            acc_on_power = X[:, acc_ix] / X[:, hpower_ix]\n",
    "            return np.c_[X, acc_on_power, acc_on_cyl] # returns a 2D array\n",
    "        \n",
    "        return np.c_[X, acc_on_cyl]\n",
    "    \n",
    "attr_adder = CustomAttrAdder(acc_on_power=True)\n",
    "data_tr_extra_attrs = attr_adder.transform(x.values)\n",
    "data_tr_extra_attrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##handling missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def num_pipeline_transformer(data):\n",
    "    '''\n",
    "    Function to process numerical transformations\n",
    "    Argument:\n",
    "        data: original dataframe \n",
    "    Returns:\n",
    "        num_attrs: numerical dataframe\n",
    "        num_pipeline: numerical pipeline object\n",
    "        \n",
    "    '''\n",
    "    numerics = ['float64', 'int64']\n",
    "\n",
    "    num_attrs = data.select_dtypes(include=numerics)\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attrs_adder', CustomAttrAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "    return num_attrs, num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical variable transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##preprocess the Origin column in data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess_origin_cols(df):\n",
    "    df[\"Origin\"] = df[\"Origin\"].map({1: \"India\", 2: \"USA\", 3: \"Germany\"})    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full data tranformation pipleine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def pipeline_transformer(data):\n",
    "    '''\n",
    "    Complete transformation pipeline for both\n",
    "    nuerical and categorical data.\n",
    "    \n",
    "    Argument:\n",
    "        data: original dataframe \n",
    "    Returns:\n",
    "        prepared_data: transformed data, ready to use\n",
    "    '''\n",
    "    cat_attrs = [\"Origin\"]\n",
    "    num_attrs, num_pipeline = num_pipeline_transformer(data)\n",
    "    full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, list(num_attrs)),\n",
    "        (\"cat\", OneHotEncoder(), cat_attrs),\n",
    "        ])\n",
    "    prepared_data = full_pipeline.fit_transform(data)\n",
    "    return prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from raw data to processed data in 2 steps\n",
    "\n",
    "raw_training_data= strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_origin_cols(raw_training_data)\n",
    "prepared_data = pipeline_transformer(preprocessed_df)\n",
    "\n",
    "prepared_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling - Regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps \n",
    "1. Import the model class\n",
    "2. Create an instance of the model class\n",
    "3. Train the model using the fit method\n",
    "4. Make prediction by passing your raw training dataset through the data processing pipeline\n",
    "5. Evaluate the result (e.g. RMSE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(prepared_data, data_labels)\n",
    "\n",
    "##testing the predictions with first 5 rows\n",
    "sample_data = data.iloc[:5]\n",
    "sample_labels = data_labels.iloc[:5]\n",
    "sample_data_prepared = pipeline_transformer(sample_data)\n",
    "\n",
    "print(\"Prediction of samples: \", lin_reg.predict(sample_data_prepared))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mpg_predictions = lin_reg.predict(prepared_data)\n",
    "lin_mse = mean_squared_error(data_labels, mpg_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, \n",
    "                         prepared_data, \n",
    "                         data_labels, \n",
    "                         scoring=\"neg_mean_squared_error\", \n",
    "                         cv = 10)\n",
    "tree_reg_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Random Forest with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##segregating the target variable from test set\n",
    "X_train = strat_train_set.drop(\"MPG\", axis=1)\n",
    "y_train = strat_train_set[\"MPG\"].copy()\n",
    "\n",
    "\n",
    "X_train_processed_df = preprocess_origin_cols(X_train)\n",
    "\n",
    "\n",
    "X_train_prepared = pipeline_transformer(X_train_processed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble  import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True,\n",
    "                           cv=10,\n",
    "                          )\n",
    "grid_search.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "extra_attrs = [\"acc_on_power\", \"acc_on_cyl\"]\n",
    "numerics = ['float64', 'int64']\n",
    "num_attrs = list(data.select_dtypes(include=numerics))\n",
    "\n",
    "attrs = num_attrs + extra_attrs\n",
    "sorted(zip(attrs, feature_importances), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire ML Pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##capturing the best configuration\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "##segregating the target variable from test set\n",
    "X_test = strat_test_set.drop(\"MPG\", axis=1)\n",
    "y_test = strat_test_set[\"MPG\"].copy()\n",
    "\n",
    "##preprocessing the test data origin column\n",
    "X_test_preprocessed = preprocess_origin_cols(X_test)\n",
    "\n",
    "##preparing the data with final transformation\n",
    "X_test_prepared = pipeline_transformer(X_test_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "##making final predictions\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print('final rmse: ', final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model as Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "##dump the model into a file\n",
    "if not os.path.isdir('model'):\n",
    "    os.mkdir('model')\n",
    "with open('model/model.pkl', 'wb') as f_out:\n",
    "    pickle.dump(final_model, f_out) # write final_model in .bin file\n",
    "    f_out.close()  # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Pickle File and using it to make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set for  3 vehicles \n",
    "\n",
    "vehicle_config = {\n",
    "    'Cylinders': [4, 6, 8],\n",
    "    'Displacement': [155.0, 160.0, 165.5],\n",
    "    'Horsepower': [93.0, 130.0, 98.0],\n",
    "    'Weight': [2500.0, 3150.0, 2600.0],\n",
    "    'Acceleration': [15.0, 14.0, 16.0],\n",
    "    'Model Year': [81, 80, 78],\n",
    "    'Origin': [3, 2, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "##loading the model from the saved file\n",
    "with open('model/model.pkl', 'rb') as f_in:\n",
    "    model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mpg(config, model):\n",
    "    \n",
    "    if type(config) == dict:\n",
    "        df = pd.DataFrame(config)\n",
    "    else:\n",
    "        df = config\n",
    "    \n",
    "    preproc_df = preprocess_origin_cols(df)\n",
    "    #print(preproc_df)\n",
    "    prepared_df = pipeline_transformer(preproc_df)\n",
    "\n",
    "    #print(len(prepared_df[0]))\n",
    "    y_pred = model.predict(prepared_df)\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mpg(vehicle_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
